{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gezi\n",
    "from gezi import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# did corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2232748/2232748 [00:08<00:00, 266428.26it/s]\n",
      "100%|██████████| 376471/376471 [00:01<00:00, 295551.94it/s]\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "  '../input/train/behaviors.tsv',\n",
    "  '../input/dev/behaviors.tsv',\n",
    "]\n",
    "\n",
    "uids = set()\n",
    "with open('../input/did_corpus.txt', 'w') as out:\n",
    "  for file in files:\n",
    "    total = len(open(file).readlines())\n",
    "    for line in tqdm(open(file), total=total):\n",
    "      l = line.strip().split('\\t')\n",
    "      uid, history = l[1], l[-2]\n",
    "      if uid in uids:\n",
    "        continue\n",
    "      uids.add(uid)\n",
    "      print(history, file=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# did corpus with test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2232748/2232748 [00:08<00:00, 271999.14it/s]\n",
      "100%|██████████| 376471/376471 [00:01<00:00, 318580.45it/s]\n",
      "100%|██████████| 2370727/2370727 [00:07<00:00, 312394.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# 效果差不多 还是采用上面那个吧 或者为了安全 只用训练数据也可以 id类安全一些 \n",
    "# 如果是引用title 等等 可以考虑都使用\n",
    "files = [\n",
    "  '../input/train/behaviors.tsv',\n",
    "  '../input/dev/behaviors.tsv',\n",
    "  '../input/test/behaviors.tsv',\n",
    "]\n",
    "\n",
    "uids = set()\n",
    "with open('../input/did_corpus2.txt', 'w') as out:\n",
    "  for file in files:\n",
    "    total = len(open(file).readlines())\n",
    "    for line in tqdm(open(file), total=total):\n",
    "      l = line.strip().split('\\t')\n",
    "      uid, history = l[1], l[-2]\n",
    "      if uid in uids:\n",
    "        continue\n",
    "      uids.add(uid)\n",
    "      print(history, file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2232748"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 710/101527 [00:00<01:40, 1001.48it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n",
      "  1%|          | 803/101527 [00:00<01:42, 978.05it/s] WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "  4%|▍         | 3941/101527 [00:04<01:54, 855.59it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "  4%|▍         | 4027/101527 [00:04<01:57, 831.05it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (512 > 512). Running this sequence through the model will result in indexing errors\n",
      " 40%|████      | 40630/101527 [00:49<01:14, 816.01it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      " 42%|████▏     | 42386/101527 [00:51<01:11, 831.01it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n",
      " 51%|█████     | 51482/101527 [01:02<01:01, 809.99it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      " 53%|█████▎    | 54232/101527 [01:06<01:03, 750.07it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
      " 62%|██████▏   | 63332/101527 [01:17<00:48, 791.96it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (549 > 512). Running this sequence through the model will result in indexing errors\n",
      " 73%|███████▎  | 74325/101527 [01:30<00:29, 908.85it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (530 > 512). Running this sequence through the model will result in indexing errors\n",
      " 87%|████████▋ | 88050/101527 [01:46<00:16, 821.47it/s] WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (635 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 101527/101527 [02:03<00:00, 821.58it/s]\n",
      "100%|██████████| 72023/72023 [00:03<00:00, 21240.62it/s]\n",
      " 85%|████████▌ | 103142/120961 [00:18<00:12, 1392.20it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (962 > 512). Running this sequence through the model will result in indexing errors\n",
      " 87%|████████▋ | 104671/120961 [00:19<00:12, 1260.50it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (898 > 512). Running this sequence through the model will result in indexing errors\n",
      " 94%|█████████▍| 113626/120961 [00:26<00:05, 1222.67it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (762 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 120961/120961 [00:32<00:00, 3682.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "files = [\n",
    "          '../input/train/news.tsv',\n",
    "          '../input/dev/news.tsv',\n",
    "          '../input/test/news.tsv'\n",
    "        ]\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "model = f'/home/gezi/data/lm/{model_name}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "vocab = gezi.Vocab(f'{model}/vocab.txt', fixed=True)\n",
    "\n",
    "dids = set()\n",
    "with open('../input/corpus.txt', 'w') as out:\n",
    "  for file_ in files:\n",
    "    total = len(open(file_).readlines())\n",
    "    for line in tqdm(open(file_), total=total):\n",
    "      l = line.strip().split('\\t')\n",
    "      did, title, abstract = l[0], l[3], l[4]\n",
    "      if did in dids:\n",
    "        continue\n",
    "      dids.add(did)\n",
    "\n",
    "      if abstract:\n",
    "        text = title + ' ' + abstract\n",
    "      else:\n",
    "        text = title\n",
    "      tokens = tokenizer.encode(text)\n",
    "      tokens =  tokens[1:-1]\n",
    "      print(' '.join(map(vocab.key, tokens)), file=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
